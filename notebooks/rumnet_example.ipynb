{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to modelling with RUMnet\n",
    "\n",
    "We reproduce in this notebook the results of the paper Reprensenting Random Uility Models with Neural Networks on the SwissMetro dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove/Add GPU use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from choice_learn.data import ChoiceDataset\n",
    "from choice_learn.models import RUMnet\n",
    "from choice_learn.datasets import load_swissmetro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are two implementation of RUMnet: One more CPU-oriented and one more GPU-oriented.\n",
    "The import of the right model is automatically done. You can also import the model directly with:\n",
    "\n",
    "```python\n",
    "from choice_learn.models import CPURUMnet, GPURUMnet\n",
    "```\n",
    "\n",
    "First, we download the SwissMetro dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_swissmetro(as_frame=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We follow the same data preparation as in the original paper in order to get the exact same results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[df.CHOICE!=0]\n",
    "choices = df.CHOICE.to_numpy() - 1\n",
    "contexts_items_availabilities = df[[\"TRAIN_AV\", \"SM_AV\", \"CAR_AV\"]].to_numpy()\n",
    "contexts_items_features = np.stack([df[[\"TRAIN_TT\", \"TRAIN_CO\", \"TRAIN_HE\"]].to_numpy(),\n",
    "                                    df[[\"SM_TT\", \"SM_CO\", \"SM_HE\"]].to_numpy(),\n",
    "                                    df[[\"CAR_TT\", \"CAR_CO\", \"CAR_HE\"]].to_numpy()], axis=1)\n",
    "# contexts_features = df[[\"GROUP\", \"PURPOSE\", \"FIRST\", \"TICKET\", \"WHO\", \"LUGGAGE\", \"AGE\", \"MALE\",\n",
    "#                         \"INCOME\", \"GA\", \"ORIGIN\", \"DEST\"]].to_numpy()\n",
    "fixed_items_features = np.eye(3)\n",
    "\n",
    "contexts_items_features[:, :, 0] = contexts_items_features[:, :, 0] / 1000\n",
    "contexts_items_features[:, :, 1] = contexts_items_features[:, :, 1] / 5000\n",
    "contexts_items_features[:, :, 2] = contexts_items_features[:, :, 2] / 100\n",
    "\n",
    "long_data = pd.get_dummies(df,\n",
    "                           columns=[\"GROUP\", \"PURPOSE\", \"FIRST\", \"TICKET\", \"WHO\",\n",
    "                                        \"LUGGAGE\", \"AGE\", \"MALE\",\n",
    "                                        \"INCOME\", \"GA\", \"ORIGIN\", \"DEST\"],\n",
    "                                        drop_first=False)\n",
    "\n",
    "# Transorming the category data into OneHot\n",
    "contexts_features = []\n",
    "for col in long_data.columns:\n",
    "    if col.startswith(\"GROUP\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"PURPOSE\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"FIRST\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"TICKET\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"WHO\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"LUGGAGE\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"AGE\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"MALE\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"INCOME\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"GA\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"ORIGIN\"):\n",
    "        contexts_features.append(col)\n",
    "    if col.startswith(\"DEST\"):\n",
    "        contexts_features.append(col)\n",
    "\n",
    "contexts_features = long_data[contexts_features].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can create our ChoiceDataset from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ChoiceDataset(fixed_items_features=(fixed_items_features.astype(\"float32\"), ),\n",
    "                        contexts_features=(contexts_features.astype(\"float32\"), ),\n",
    "                        contexts_items_features=(contexts_items_features.astype(\"float32\"), ),\n",
    "                        contexts_items_availabilities=contexts_items_availabilities,\n",
    "                        choices=choices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's Cross-Validate !\n",
    "We keep a scikit-learn-like structure.\n",
    "To avoid creating dependancies, we use a different train/test split code, but the following would totally work:\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "rs = ShuffleSplit(n_splits=5, test_size=.2, random_state=0)\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(rs.split(dataset.choices)):\n",
    "    train_dataset = dataset[train_index]\n",
    "    test_dataset = dataset[test_index]\n",
    "\n",
    "    model = RUMnet(**args)\n",
    "    model.instantiate()\n",
    "    model.fit(train_dataset)\n",
    "    model.evaluate(test_dataset)\n",
    "```\n",
    "\n",
    "We just use a numpy based split, but the core code is the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = {\n",
    "    \"num_products_features\": 6,\n",
    "    \"num_customer_features\": 83,\n",
    "    \"width_eps_x\": 20,\n",
    "    \"depth_eps_x\": 5,\n",
    "    \"heterogeneity_x\": 10,\n",
    "    \"width_eps_z\": 20,\n",
    "    \"depth_eps_z\": 5,\n",
    "    \"heterogeneity_z\": 10,\n",
    "    \"width_u\": 20,\n",
    "    \"depth_u\": 5,\n",
    "    \"tol\": 1,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"lr\": 0.0002,\n",
    "    \"logmin\": 1e-10,\n",
    "    \"label_smoothing\": 0.02,\n",
    "    \"callbacks\": [],\n",
    "    \"epochs\": 150,\n",
    "    \"batch_size\": 128,\n",
    "    \"tol\": 1e-5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.permutation(list(range(len(dataset))))\n",
    "\n",
    "fit_losses = []\n",
    "test_eval = []\n",
    "for i in range(5):\n",
    "    test_indexes = indexes[int(len(indexes) * 0.2 * i):int(len(indexes) * 0.2 * (i + 1))]\n",
    "    train_indexes = np.concatenate([indexes[:int(len(indexes) * 0.2 * i)],\n",
    "                                    indexes[int(len(indexes) * 0.2 * (i + 1)):]],\n",
    "                                   axis=0)\n",
    "\n",
    "    train_dataset = dataset[train_indexes]\n",
    "    test_dataset = dataset[test_indexes]\n",
    "\n",
    "    model = RUMnet(**model_args)\n",
    "    model.instantiate()\n",
    "\n",
    "    losses = model.fit(train_dataset, val_dataset=test_dataset)\n",
    "    probas = model.predict_probas(test_dataset)\n",
    "    eval = tf.keras.losses.CategoricalCrossentropy(from_logits=False)(y_pred=model.predict_probas(test_dataset), y_true=tf.one_hot(test_dataset.choices, 3))\n",
    "    test_eval.append(eval)\n",
    "    print(test_eval)\n",
    "\n",
    "    fit_losses.append(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(fit_losses)):\n",
    "    plt.plot(fit_losses[i][\"train_loss\"], label=f\"fold {i}\", c=[\"r\", \"g\", \"b\", \"cyan\", \"purple\"][i])\n",
    "    plt.plot(fit_losses[i][\"test_loss\"], c=[\"r\", \"g\", \"b\", \"cyan\", \"purple\"][i])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average LogLikeliHood on test:\", np.mean(test_eval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
