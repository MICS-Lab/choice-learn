{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to choice-learn's modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChoiceModel - Getting Started\n",
    "\n",
    "choice-learn package offers a high level API to conceive and estimate discrete choice models.\n",
    "\n",
    "We begin this tutorial with the estimation of a Conditional Logit Model on the ModeCanada dataset.\n",
    "\n",
    "First, we download our data as a ChoiceDataset. See the data management tutorial first if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.datasets import load_modecanada\n",
    "\n",
    "transport_df = load_modecanada(as_frame=True)\n",
    "\n",
    "# Following torch-choice guide:\n",
    "transport_df = transport_df.loc[transport_df.noalt == 4]\n",
    "\n",
    "items = [\"air\", \"bus\", \"car\", \"train\"]\n",
    "\n",
    "transport_df[\"oh_air\"] = transport_df.apply(lambda row: 1. if row.alt == items[0] else 0., axis=1)\n",
    "transport_df[\"oh_bus\"] = transport_df.apply(lambda row: 1. if row.alt == items[1] else 0., axis=1)\n",
    "transport_df[\"oh_car\"] = transport_df.apply(lambda row: 1. if row.alt == items[2] else 0., axis=1)\n",
    "transport_df[\"oh_train\"] = transport_df.apply(lambda row: 1. if row.alt == items[3] else 0., axis=1)\n",
    "\n",
    "transport_df.income = transport_df.income.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.data import ChoiceDataset\n",
    "dataset = ChoiceDataset.from_single_df(df=transport_df,\n",
    "                                       fixed_items_features_columns=[\"oh_air\", \"oh_bus\", \"oh_car\", \"oh_train\"],\n",
    "                                       contexts_features_columns=[\"income\"],\n",
    "                                       contexts_items_features_columns=[\"cost\", \"freq\", \"ovt\", \"ivt\"],\n",
    "                                       items_id_column=\"alt\",\n",
    "                                       contexts_id_column=\"case\",\n",
    "                                       choices_column=\"choice\",\n",
    "                                       choice_mode=\"one_zero\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can import the needed modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models import ConditionalMNL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional MNL \\ref{Train} specifies a linear utility for each item i during the session s with regards to the features:\n",
    "$$\n",
    "U(i, s) = \\sum_{features} a(i, s) * feat(i, s)\n",
    "$$\n",
    "\n",
    "We will use a ModelSpecification object to define our model with regards to our ChoiceDataset.\n",
    "For each feature in the choice dataset we can specify how it must be specified in the utility.\n",
    "\n",
    "Let's re-use a common example from \\ref{} on the ModeCanda dataset:\n",
    "$$\n",
    "U(i, s) = \\beta^{inter}_i + \\beta^{price} \\cdot price(i, s) + \\beta^{freq} \\cdot freq(i, s) + \\beta^{ovt} \\cdot ovt(i, s) + \\beta^{income}_i \\cdot income(s) + \\beta^{ivt}_i \\cdot ivt(i, t) + \\epsilon(i, t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we want to estimate:\n",
    "\n",
    "- one $\\beta^{price}$, $\\beta^{freq}$ and $\\beta^{ovt}$ coefficient. They are shared by all items.\n",
    "- one $\\beta^{ivt}$ coefficient for **each** item.\n",
    "- one $\\beta^{inter}$ and $\\beta^{income}$ coefficient for **each** item, with **additional constraint** to be 0 for the first item (air).\n",
    "\n",
    "We will use a ModelSpecification object to create the right utility function.\n",
    "We need to specify for each weight:\n",
    "- a unique name\n",
    "- the name of the feature it goes with:\n",
    "    - it must match the feature name in the ChoiceDataset\n",
    "    - \"intercept\" is the standardized name used for intercept, pay attention not to override it\n",
    "- items_indexes: the items concerned, as indexed in the ChoiceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "model = ConditionalMNL(optimizer=\"lbfgs\")\n",
    "\n",
    "# Creation of the different weights:\n",
    "\n",
    "\n",
    "# add_coefficients adds one coefficient for each specified item_index\n",
    "# intercept, and income are added for each item except the first one that needs to be zeroed\n",
    "model.add_coefficients(coefficient_name=\"beta_inter\", feature_name=\"intercept\", items_indexes=[1, 2, 3])\n",
    "model.add_coefficients(coefficient_name=\"beta_income\", feature_name=\"income\", items_indexes=[1, 2, 3])\n",
    "\n",
    "# ivt is added for each item:\n",
    "model.add_coefficients(coefficient_name=\"beta_ivt\", feature_name=\"ivt\", items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# shared_coefficient add one coefficient that is used for all items specified in the items_indexes:\n",
    "# Here, cost, freq and ovt coefficients are shared between all items\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_cost\", feature_name=\"cost\", items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_freq\", feature_name=\"freq\", items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(coefficient_name=\"beta_ovt\", feature_name=\"ovt\", items_indexes=[0, 1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can instantiate our ConditionalMNL from the specification. We use LBFGS as the estimation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the the coefficients values, use the .fit method with the ChoiceDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, n_epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to see the estimated coefficients with the .weights argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative loglikelihood can be estimated using .evaluate():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average neg-loglikelihood is:\", model.evaluate(dataset).numpy())\n",
    "print(\"The total neg-loglikelihood is:\", model.evaluate(dataset).numpy()*len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A faster specification can be done using a dictionnary. It follows torch-choice \\ref{} method to create conditional logit models.\n",
    "The parameters dict needs to be as follows:\n",
    "- The key is the feature name\n",
    "- The value is the mode. Currently three modes are available:\n",
    "    - constant: the learned coefficient is shared by all items\n",
    "    - item: one coefficient by item is estimated, the value for the item at index 0 is set to 0\n",
    "    - item-full: one coefficient by item is estimated\n",
    "\n",
    "In order to create the same model for the ModeCanada dataset, it looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation of the parameters dictionnary\n",
    "params = {\"income\": \"item\",\n",
    " \"cost\": \"constant\", \n",
    " \"freq\": \"constant\",\n",
    " \"ovt\": \"constant\", \n",
    " \"ivt\": \"item-full\",\n",
    " \"intercept\": \"item\"}\n",
    "\n",
    "# Instantiation of the model\n",
    "cmnl = ConditionalMNL(parameters=params, optimizer=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cmnl.fit(dataset, n_epochs=1000)\n",
    "print(cmnl.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the estimated coefficients and the negative log-likelihood obtained in \\ref{} and \\ref{torch-choice}, and it is similar !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Here are the values obtained in the references:\n",
    "gt_weights = [\n",
    "    tf.constant([[-0.0890796, -0.0279925, -0.038146]]),\n",
    "    tf.constant([[-0.0333421]]),\n",
    "    tf.constant([[0.0925304]]),\n",
    "    tf.constant([[-0.0430032]]),\n",
    "    tf.constant([[0.0595089, -0.00678188, -0.00645982, -0.00145029]]),\n",
    "    tf.constant([[0.697311, 1.8437, 3.27381]]),\n",
    "]\n",
    "gt_model = ConditionalMNL(parameters=params, lr=0.01)\n",
    "gt_model.fit(dataset, n_epochs=1, batch_size=-1)\n",
    "\n",
    "# Here we estimate the negative log-likelihood with these coefficients (also, we obtain same value as in those papers):\n",
    "gt_model.weights = gt_weights\n",
    "gt_model.evaluate(dataset) * len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to estimate the utilities, use the .predict_utility() method. In order to estimate the probabilities, use the .compute_probabilities() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Utilities of each item for the first 5 sessions:\", cmnl.predict_utility(dataset)[:5])\n",
    "print(\"Purchase probability of each item for the first 5 sessions:\", cmnl.predict_probas(dataset)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For very large datasets that do not fit entirely in the memory, the LBFGS method might not be the best choice. Here we can use the power of the Tensorflow library to use stochastic gradient descent optimizers.\n",
    "\n",
    "In this case, it is possible to obtain the same coefficients estimation, also it is a little tricky to get it quickly. We need to adjust the learning rate over time for the optimization not to be too slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl = ConditionalMNL(parameters=params, optimizer=\"Adam\")\n",
    "history = cmnl.fit(dataset, n_epochs=2000, batch_size=-1)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr / 5\n",
    "history2 = cmnl.fit(dataset, n_epochs=4000, batch_size=-1)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr  / 10\n",
    "history3 = cmnl.fit(dataset, n_epochs=20000, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl.evaluate(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
