{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to choice-learn's modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- [Example 1: ConditionalMNL with Swissmetro](#example-1-swissmetro)\n",
    "    - [A few words on c-MNL formulation](#the-conditional-logit-model)\n",
    "    - [Instantiation and estimation with Choice-Learn](#conditional-logit-estimation-with-choice-learn)\n",
    "- [Example 2: ConditionalMNL with ModeCanda](#example-2-the-modecanada-dataset)\n",
    "    - [Utility Formulation](#utility-formulation)\n",
    "    - [Model Specification](#model-formulation)\n",
    "    - [Lighter method for specification](#faster-specification)\n",
    "    - [Comparison with other implementations](#comparison-with-other-implementations-results)\n",
    "    - [Get utility estimation and probabilities](#estimate-utility--probabilities)\n",
    "    - [Using Gradient Descent Optimizers](#using-gradient-descent-optimizers)\n",
    "    \n",
    "\n",
    "For model customization and more explanation on ChoiceModel and the endpoints, you can go [here](./custom_model.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: SwissMetro\n",
    "\n",
    "The choice-learn package offers a high level API to conceive and estimate discrete choice models. Several models are ready to be used, you can check the list [here](../README.md). If you want to create your own model or another one that is not in the list, the lower level API can help you. Check the notebook [here](./4_model_customization.ipynb).\n",
    "\n",
    "Let's begin this tutorial with the estimation of a Conditional Logit Model on the SwissMetro dataset[3].\n",
    "It follows the specifications described in [PyLogit](https://github.com/timothyb0912/pylogit/blob/master/examples/notebooks/Main%20PyLogit%20Example.ipynb) and [Biogeme](https://github.com/mncosta/biogeme_tutorial/blob/master/6.1-MultinomialLogitAndProbitModels/01-biogeme-basics.ipynb).\n",
    "\n",
    "\n",
    "First, we download our data as a ChoiceDataset. See the [data management tutorial](./choice_learn_introduction_data.ipynb) first if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.datasets import load_swissmetro\n",
    "swiss_dataset = load_swissmetro(preprocessing=\"tutorial\")\n",
    "print(swiss_dataset.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Conditional Logit model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional Logit [2] specifies a linear utility for each features of item $i$:\n",
    "$$\n",
    "U(i) = \\sum_{features} a(i) * feature(i)\n",
    "$$\n",
    "The probability to choose $i$ among the set of available alternatives $\\mathcal{A}$ is then:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(i) = \\frac{e^{U(i)}}{\\sum_{j \\in \\mathcal{A}} e^{U(j)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the SwissMetro dataset we are trying to predict a customer mean of transport among train, swissmetro and car from the features:\n",
    "- TT (transit time)\n",
    "- CO (cost)\n",
    "- HE (headway)\n",
    "- Survey (where the survey took place)\n",
    "- Luggage number\n",
    "- seats configuration in the swissmetro\n",
    "- first class or not\n",
    "\n",
    "An important step is to define the right utility function for the model to fit well the dataset. Let's take the following formulation defined in PyLogit:\n",
    "\n",
    "- $\n",
    "U(train) = \\beta_{train}^{inter} + \\beta^{tt}_{train/sm} \\cdot TT(train) + \\beta^{co}_{train} \\cdot CO(train) + \\beta^{he}_{train} \\cdot HE(train) + \\beta^{survey} \\cdot SV(train)\n",
    "$\n",
    "\n",
    "- $\n",
    "U(sm) = \\beta_{sm}^{inter} + \\beta^{tt}_{train/sm} \\cdot TT(sm) + \\beta^{co}_{sm} \\cdot CO(sm) + \\beta^{he}_{sm} \\cdot HE(sm) + \\beta^{survey} \\cdot SV(sm) + \\beta^{seat} \\cdot SEAT(sm) + \\beta^{first\\_class} \\cdot FC(sm)\n",
    "$\n",
    "\n",
    "- $\n",
    "U(car) = \\beta^{tt}_{car} \\cdot TT(car) + \\beta^{co}_{car} \\cdot CO(car) + \\beta^{luggage==1} \\cdot \\mathbb{1}_{Luggage==1} + \\beta^{luggage>1} \\cdot \\mathbb{1}_{Luggage>1}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we want to estimate:\n",
    "\n",
    "- one $\\beta^{tt}_{train/sm}$ **shared** train and sm items and one $\\beta^{tt}_{car}$ for the car item\n",
    "- one $\\beta^{co}$ coefficient for **each** item.\n",
    "- one $\\beta^{inter}$ and $\\beta^{he}$ for train and sm, and zeroed for the car alternative\n",
    "- one $\\beta^{survey}$, $\\beta^{seat}$, $\\beta^{first\\_class}$, $\\beta^{luggage==1}$ and $\\beta^{luggage>1}$ shared or not by different items\n",
    "\n",
    "To build a model, we need to specify for each weight $\\beta$:\n",
    "- the name of the feature it goes with:\n",
    "    - it must match the feature name in the ChoiceDataset\n",
    "    - \"intercept\" is the standardized name used for intercept, pay attention not to override it\n",
    "- items_indexes: the items concerned, as indexed in the ChoiceDataset\n",
    "- (optionally) a unique weight name\n",
    "\n",
    "> **Attention**\n",
    "> \n",
    "> *add_coefficients* is to be used to get one coefficient by given items_indexes\\\n",
    "> *add_shared_coefficients* is to be used to get on coefficient that is used for utility of all given items_indexes\n",
    "\n",
    "Here is how to create a model following our defined utility functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Logit Estimation with Choice-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models import ConditionalMNL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization of the model\n",
    "swiss_model = ConditionalMNL(optimizer=\"lbfgs\")\n",
    "\n",
    "# Intercept for train & sm\n",
    "swiss_model.add_coefficients(feature_name=\"intercept\", items_indexes=[0, 1])\n",
    "# beta_he for train & sm\n",
    "swiss_model.add_coefficients(feature_name=\"headway\",\n",
    "                             items_indexes=[0, 1],\n",
    "                             coefficient_name=\"beta_he\")\n",
    "# beta_co for all items\n",
    "swiss_model.add_coefficients(feature_name=\"cost\",\n",
    "                             items_indexes=[0, 1, 2])\n",
    "# beta first_class for train\n",
    "swiss_model.add_coefficients(feature_name=\"regular_class\",\n",
    "                             items_indexes=[0])\n",
    "# beta seats for train\n",
    "swiss_model.add_coefficients(feature_name=\"seats\", items_indexes=[1])\n",
    "# betas luggage for car\n",
    "swiss_model.add_coefficients(feature_name=\"single_luggage_piece\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_luggage=1\")\n",
    "swiss_model.add_coefficients(feature_name=\"multiple_luggage_piece\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_luggage>1\")\n",
    "# beta TT only for car\n",
    "swiss_model.add_coefficients(feature_name=\"travel_time\",\n",
    "                             items_indexes=[2],\n",
    "                             coefficient_name=\"beta_tt_car\")\n",
    "\n",
    "# betas TT and HE shared by train and sm\n",
    "swiss_model.add_shared_coefficient(feature_name=\"travel_time\",\n",
    "                                   items_indexes=[0, 1])\n",
    "swiss_model.add_shared_coefficient(feature_name=\"train_survey\",\n",
    "                                   items_indexes=[0, 1],\n",
    "                                   coefficient_name=\"beta_survey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation of the model\n",
    "history = swiss_model.fit(swiss_dataset, get_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Onece the model is estimated, we can look at the weights with the .trainable_weights argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'beta_intercept:0' shape=(1, 2) dtype=float32, numpy=array([[-1.2929311, -0.5025746]], dtype=float32)>,\n",
       " <tf.Variable 'beta_he:0' shape=(1, 2) dtype=float32, numpy=array([[-0.31433567, -0.37731853]], dtype=float32)>,\n",
       " <tf.Variable 'beta_cost:0' shape=(1, 3) dtype=float32, numpy=array([[-0.56176245, -0.28167567, -0.5138463 ]], dtype=float32)>,\n",
       " <tf.Variable 'beta_regular_class:0' shape=(1, 1) dtype=float32, numpy=array([[0.5650174]], dtype=float32)>,\n",
       " <tf.Variable 'beta_seats:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7824476]], dtype=float32)>,\n",
       " <tf.Variable 'beta_luggage=1:0' shape=(1, 1) dtype=float32, numpy=array([[0.4227598]], dtype=float32)>,\n",
       " <tf.Variable 'beta_luggage>1:0' shape=(1, 1) dtype=float32, numpy=array([[1.4139806]], dtype=float32)>,\n",
       " <tf.Variable 'beta_tt_car:0' shape=(1, 1) dtype=float32, numpy=array([[-0.7229834]], dtype=float32)>,\n",
       " <tf.Variable 'beta_travel_time:0' shape=(1, 1) dtype=float32, numpy=array([[-0.69901353]], dtype=float32)>,\n",
       " <tf.Variable 'beta_survey:0' shape=(1, 1) dtype=float32, numpy=array([[2.542476]], dtype=float32)>]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swiss_model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily acces the negative log likelihood value for the training dataset or another one using the .evaluate() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5156.3345>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swiss_dataset) * swiss_model.evaluate(swiss_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If yoou set get_report to True in .fit, the model automatically creates a report for each of the coefficient, with its estimation, its standard deviation and more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swiss_model.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the same results (estimation of parameters and negative log-likelihood) as the PyLogit package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #2: the ModeCanada Dataset\n",
    "\n",
    "\n",
    "### Utility formulation\n",
    "\n",
    "Let's reproduce a common example from [Torch-Choice](https://gsbdbi.github.io/torch-choice/conditional_logit_model_mode_canada/) on the ModeCanada [1] dataset:\n",
    "$$\n",
    "U(i, c) = \\beta^{inter}_i + \\beta^{price} \\cdot price(i, c) + \\beta^{freq} \\cdot freq(i, c) + \\beta^{ovt} \\cdot ovt(i, c) + \\beta^{income}_i \\cdot income(c) + \\beta^{ivt}_i \\cdot ivt(i, c) + \\epsilon(i, c)\n",
    "$$\n",
    "\n",
    "A description of the dataset and the features can be found in [1]. We want to predict the future mean of transport from [train, air, bus, car] using frequence, price, in-vehicule transport time (ivt) and out-of-vehicule transport time (ovt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to check what's in the dataset:\n",
    "from choice_learn.datasets import load_modecanada\n",
    "\n",
    "transport_df = load_modecanada(as_frame=True)\n",
    "transport_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate:\n",
    "\n",
    "- one $\\beta^{price}$, $\\beta^{freq}$ and $\\beta^{ovt}$ coefficient. They are **shared** by all items.\n",
    "- one $\\beta^{ivt}$ coefficient for **each** item.\n",
    "- one $\\beta^{inter}$ and $\\beta^{income}$ coefficient for **each** item, with **additional constraint** to be 0 for the first item (air).\n",
    "\n",
    "One notes that it makes sense to include an intercept $\\beta^{inter}$ for each item since $ivt(i, c)$ and $income(c)$ depends on each choice $c$.\n",
    "\n",
    "Additionally to previous example we manually specify the weights names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the ChoiceDataset\n",
    "canada_dataset = load_modecanada(as_frame=False, preprocessing=\"tutorial\")\n",
    "\n",
    "print(canada_dataset.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from choice_learn.models import ConditionalMNL\n",
    "\n",
    "# Initialization of the model\n",
    "model = ConditionalMNL()\n",
    "\n",
    "# Creation of the different weights:\n",
    "\n",
    "# shared_coefficient add one coefficient that is used for all items specified in the items_indexes:\n",
    "# Here, cost, freq and ovt coefficients are shared between all items\n",
    "model.add_shared_coefficient(feature_name=\"cost\", items_indexes=[0, 1, 2, 3])\n",
    "# You can specify you own coefficient name\n",
    "model.add_shared_coefficient(feature_name=\"freq\",\n",
    "                             coefficient_name=\"beta_frequence\",\n",
    "                             items_indexes=[0, 1, 2, 3])\n",
    "model.add_shared_coefficient(feature_name=\"ovt\", items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# ivt is added for each item:\n",
    "model.add_coefficients(feature_name=\"ivt\", items_indexes=[0, 1, 2, 3])\n",
    "\n",
    "# add_coefficients adds one coefficient for each specified item_index\n",
    "# intercept, and income are added for each item except the first one that needs to be zeroed\n",
    "model.add_coefficients(feature_name=\"intercept\", items_indexes=[1, 2, 3])\n",
    "model.add_coefficients(feature_name=\"income\", items_indexes=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(canada_dataset, get_report=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The average neg-loglikelihood is:\", model.evaluate(canada_dataset).numpy())\n",
    "print(\"The total neg-loglikelihood is:\", model.evaluate(canada_dataset).numpy()*len(canada_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Faster Specification\n",
    "\n",
    "A faster specification can be done using a dictionnary. It follows torch-choice method to create conditional logit models.\n",
    "The parameters dict needs to be as follows:\n",
    "- The key is the feature name\n",
    "- The value is the mode. Currently three modes are available:\n",
    "    - constant: the learned coefficient is shared by all items\n",
    "    - item: one coefficient by item is estimated, the value for the item at index 0 is set to 0\n",
    "    - item-full: one coefficient by item is estimated\n",
    "\n",
    "In order to create the same model for the ModeCanada dataset, it looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation with the coefficients dictionnary\n",
    "coefficients = {\"income\": \"item\",\n",
    " \"cost\": \"constant\",\n",
    " \"freq\": \"constant\",\n",
    " \"ovt\": \"constant\",\n",
    " \"ivt\": \"item-full\",\n",
    " \"intercept\": \"item\"}\n",
    "\n",
    "# Instantiation of the model\n",
    "cmnl = ConditionalMNL(coefficients=coefficients, epochs=1000, optimizer=\"lbfgs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cmnl.fit(canada_dataset)\n",
    "print(cmnl.trainable_weights)\n",
    "print(cmnl.evaluate(canada_dataset).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with other implementations results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Here are the values obtained in the references:\n",
    "gt_weights = [\n",
    "    tf.constant([[-0.0890796, -0.0279925, -0.038146]]),\n",
    "    tf.constant([[-0.0333421]]),\n",
    "    tf.constant([[0.0925304]]),\n",
    "    tf.constant([[-0.0430032]]),\n",
    "    tf.constant([[0.0595089, -0.00678188, -0.00645982, -0.00145029]]),\n",
    "    tf.constant([[0.697311, 1.8437, 3.27381]]),\n",
    "]\n",
    "gt_model = ConditionalMNL(coefficients=coefficients)\n",
    "gt_model.instantiate(canada_dataset)\n",
    "canada_dataset\n",
    "# Here we estimate the negative log-likelihood with these coefficients (also, we obtain same value as in those papers):\n",
    "gt_model.trainable_weights = gt_weights\n",
    "print(\"'Ground Truth' Negative LogLikelihood:\", gt_model.evaluate(canada_dataset) * len(canada_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate Utility & probabilities\n",
    "In order to estimate the utilities, use the .predict_utility() method. In order to estimate the probabilities, use the .predict_probas() method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Utilities of each item for the first 5 sessions:\", cmnl.compute_batch_utility(*canada_dataset.batch[:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Purchase probability of each item for the first 5 sessions:\", cmnl.predict_probas(canada_dataset)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gradient Descent Optimizers\n",
    "\n",
    "For very large datasets that do not fit entirely in the memory, we have to work with data batches. In this case, the LBFGS method is usually not the best choice. In those cases, we will prefer stochastic gradient descent optimizers.\n",
    "\n",
    "In this case, it is possible to obtain the same coefficients estimation, also it is a little tricky to get it quickly. We need to adjust the learning rate over time for the optimization not to be too slow.\n",
    "L-BFGS is more efficient for small dataset - Gradient Descent for large ones !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl = ConditionalMNL(coefficients=coefficients, optimizer=\"Adam\", epochs=2000, batch_size=-1)\n",
    "history = cmnl.fit(canada_dataset)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr / 5\n",
    "cmnl.epochs = 4000\n",
    "history2 = cmnl.fit(canada_dataset)\n",
    "cmnl.optimizer.lr = cmnl.optimizer.lr  / 10\n",
    "cmnl.epochs = 20000\n",
    "history3 = cmnl.fit(canada_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to look at the loss (negative loglikelyhood) over time to see how the estimation goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history[\"train_loss\"])\n",
    "plt.title(\"First part of the gradient descent.\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history2[\"train_loss\"] + history3[\"train_loss\"])\n",
    "plt.title(\"Second and third part of the gradient descent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnl.evaluate(canada_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] ModeCanada dataset in *Application and interpretation of nested logit models of intercity mode choice*, Christophier, V. F.; Koppelman, S. (1993)\\\n",
    "[2] Conditional MultinomialLogit, Train, K.; McFadden, D.; Ben-Akiva, M. (1987)\\\n",
    "[3] Siwssmetro dataset in *The acceptance of modal innovation: The case of Swissmetro*, Bierlaire, M.; Axhausen, K.; Abay, G (2001)\\"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
